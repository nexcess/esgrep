#!/usr/bin/env python

import time
import argparse
import sys
import os
import errno
import yaml
import json
import elasticsearch
import elasticsearch.helpers

try:
    config = yaml.load(file('/etc/esgrep/esgrep.yml', 'r'))
except yaml.YAMLError as err:
    print('Error in config file:', err)

if 'default_index' not in config:
    config['default_index'] = 'logstash'

parser = argparse.ArgumentParser(prog='esgrep')
parser.add_argument('-i', '--index',
                    default='{0}-{1}'.format(
                        config['default_index'],
                        time.strftime('%Y.%m.%d')),
                    help=('index to search against. Supports wildcards' +
                          ' (e.g., {0}-2016.11.01, {0}-2016.10.*,' +
                          'etc). defaults to {0}-YYYY-MM-DD').format(
                        config['default_index']
                    )

                    )
parser.add_argument('query',
                    default='*',
                    help=('query string query to pass to Elasticsearch, ' +
                          'OR a file containing an Elasticsearch query ' +
                          '(using the full Elasticsearch query DSL)')
                    )
parser.add_argument('-t', '--timeout',
                    default=30.0,
                    help='how long to wait for a response from Elasticsearch (defaults to 30 seconds)'
                    )
parser.add_argument('-j', '--json',
                    action='store_true',
                    help='return all (_source) fields as json'
                    )
parser.add_argument('-a', '--agg',
                    action='store_true',
                    help='return only aggregations from query'
                    )
parser.add_argument('-ts', '--timestart',
                    default=False,
                    help='starting timestamp to filter query results by'
                    )
parser.add_argument('-te', '--timeend',
                    default='now',
                    help='ending timestamp to filter query results by'
                    )
parser.add_argument('--timefield',
                    default='@timestamp',
                    help='field used when applying a time range to a search ' +
                         '(defaults to "@timestamp")'
                    )
parser.add_argument('-s', '--sort',
                    default='@timestamp',
                    help='comma separated list of fields to sort by ' +
                         '(defaults to "@timestamp")'
                    )
parser.add_argument('-f', '--fields',
                    default='*,_source',
                    help='comma separated list of fields to search ' +
                         '(defaults to "*,_source")'
                    )

args = parser.parse_args()


def get_scroller(es, query, args):
    """ construct and return a scroller for a given query """
    return elasticsearch.helpers.scan(
        es,
        query,
        index=args.index,
        request_timeout=float(args.timeout),
        preserve_order=True
    )


def get_aggs(es, query, args):
    """ run a query and return only the queries aggregations """
    return es.search(index=args.index,
                     body=query,
                     timeout=float(args.timeout),
                     filter_path=['aggregations'])


# If args.query a file, parse as json and use
# contents as the query passed to Elasticsearch
#
# elsif args.query is used to build a query string query
if os.path.isfile(args.query):
    with open(args.query) as file:
        query = json.loads(file.read())
else:
    time_range = {
        args.timefield: {
            'lte': args.timeend
        }
    }
    if args.timestart:
        time_range[args.timefield]['gte'] = args.timestart

    query = {
        'query': {
            'bool': {
                'must': [
                    {
                        'range': time_range,
                    },
                    {
                        'query_string': {
                            'query': args.query,
                            'analyze_wildcard': 'true'
                        }
                    }
                ]
            }
        },
        'sort': args.sort.split(','),
        'fields': args.fields.split(','),
    }

es = elasticsearch.Elasticsearch(hosts=config['es_nodes'])

# We either return any aggregations from the search,
# or scroll through the returned hits
if args.agg:
    result = get_aggs(es, query, args)
    agg_json = json.dumps(result['aggregations'])
    try:
        print(agg_json)
    except IOError as err:
        if err.errno == errno.EPIPE:
            sys.exit(0)
else:
    scroller = get_scroller(es, query, args)
    try:
        for doc in scroller:
            if args.json:
                msg = doc['_source']
                if 'cmdline' in msg:
                    msg['cmdline'] = " ".join(msg['cmdline']).strip()
                if 'message' in doc['_source']:
                    try:
                        msg['message'] = json.loads(doc['_source']['message'])
                        if 'cmdline' in msg['message']:
                            msg['message']['cmdline'] = " ".join(msg['message']['cmdline']).strip()
                    except ValueError, e:
                        pass
                print json.dumps(msg)
            else:
                print '{0} {1} {2} {3}'.format(
                    doc['_source']['@timestamp'],
                    doc['_source']['host'],
                    doc['_source']['program'],
                    doc['_source']['message']
                )
    except IOError as err:
        if err.errno == errno.EPIPE:
            sys.exit(0)
